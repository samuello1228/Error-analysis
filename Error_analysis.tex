\documentclass[12pt,a4paper]{report}
%\documentstyle[epsfig]{article}
\textheight=700pt
\textwidth=510pt
\oddsidemargin=-30pt
\topmargin=-35pt
\usepackage{graphicx,wrapfig,epstopdf}
\usepackage[nottoc]{tocbibind}
\usepackage[]{amsmath}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textgreek}
\usepackage{csquotes}
\usepackage{titlesec}
\usepackage{breqn}
\usepackage{hyperref}
\setcounter{chapter}{1}

\begin{document}
\begin{center}
{\bf Advanced Lab's course} \\ %\vspace{2mm}
Introduction to Statistical Analysis \\
and Error Analysis \\
\end{center}


\section{Aim of the lab}
This lab allows students to meet several goals:
\begin{enumerate}
\item Introduction to error analysis and statistical analysis of the data
\item Introduction to basics of analysis and simulations with \textbf{Matlab} (or any other software used to do data analysis)
\item Performance of basic measurements of data with low and large statistics
\end{enumerate}


\section{Introduction}
Data analysis and statistical methods play an essential role in science as it is a tool which gives possibility to treat uncertainties to data and draw conclusions. For experimental physics, it is also a tool to design and plan an experiment. Statistical methods of data analysis allow to identify independent and depended variables in the system, decide which model (theory) should be chosen for data description and etc. This laboratory manual is aimed to give students a brief introduction to data analysis techniques, however it is highly recommended to read further literature \cite{stat_1, stat_2, stat_3}.

\subsection{Types of Error}
The term `error' refers to any difference between the observed value of any physical quantity and the `actual' value.
There are different types of error:
\begin{enumerate}
\item Human error: due to the observational reasons by human
\item Systematic error: due to the experimental instruments and it determins the accuracy of the reading.
\item Random error: due to the environmental reasons or the nature of the measurement (e.g. radioactive decay)
\end{enumerate}

\section{Elements of probability theory: distribution, moments of distribution, types of distribution}
During the experiment one deals with stochastic processes, during which each next trial is independent on previous result. Random processes are described by the \textit{probability density} function $P(x)$ (term \textit{distribution} function is also used). Depending on the nature of variable \enquote{x}, distribution $P(x)$ can be continuous or discrete. If variable \enquote{x} is discrete, then the probability to obtain \enquote{$x_i$} equals to $P(x_i)$. If variable \enquote{x} is continuous, then the probability to obtain \enquote{x} in the region from $x$ to $x+dx$ equals to $P(x)dx$. \\
Usually, distribution probability is normalised to 1. In case of continuous distribution:
\begin{equation*}
\int_{-\infty}^{+\infty} P(x)dx = 1
\end{equation*}
In case of discrete distribution:
\begin{equation*}
\sum_{i} P(x_i) = 1
\end{equation*}
The probability $p(x)$ to find $x$ between certain values in case of continuous distribution is defined in the following way:
\begin{equation*}
p(x_1 \leq x \leq x_2) = \int_{x_1}^{x_2} P(x)dx
\end{equation*}
In case of discrete distribution, integration should be replaced with summation:
\begin{equation*}
p(x_1 \leq x \leq x_2) = \sum_{x = x_1}^{x_2} P(x)
\end{equation*}

\subsection{Moments of distribution}
Theoretical mean value $\mu$ of variable $x$ is defined as the first moment about zero of the distribution $P(x)$. Mathematically it can be expressed in the following way:
\begin{equation} \label{first_moment}
\mu = E[x] = \int x P(x) dx
\end{equation}
The variance of the distribution $\sigma^2$ is defined as a second moment about the mean value:
\begin{equation} \label{second_moment}
\sigma^2 = E[(x-\mu)^2] = \int (x-\mu)^2P(x)dx
\end{equation}
It has a meaning of average squared deviation of variable \enquote{x} from its mean value $\mu$. Square root of variance, $\sigma$, is called standard deviation, and used to describe the width of the distribution. Value of $\sigma$ allows to judge about the magnitude of fluctuation of random variable \enquote{x} around its mean value $\sigma$. Also one can introduce higher moments of distribution, but they are rarely used in experimental physics.

Another important definition which we will make use of later is the expectation value of a random variable $E[x]$ or a random variable function $E[f(x)]$.
\begin{equation} \label{expectation_value}
E[f(x)] =  \int f(x) P(x) dx
\end{equation}

In the case of discrete distributions, integration in equations \ref{first_moment}, \ref{second_moment} and \ref{expectation_value} should be changed to summation.
\begin{equation*}
E[f(x)] =  \sum_i f(x_i) P(x_i)
\end{equation*}
\begin{equation*}
\mu = E[x] = \sum_i x_i P(x_i)
\end{equation*}
\begin{equation*}
\sigma^2 = E[(x-\mu)^2] = \sum_i (x_i-\mu)^2P(x_i)
\end{equation*}
Students are welcome to study literature for more information about it \cite{stat_1, stat_2, stat_3}.

In the case of random multi-variable $(x_1,x_2,\dots,x_n)$ with a multivariate distribution $P(x_1,x_2,\dots,x_n)$, the definition of expectation value can be generalized.
\begin{align*}
E[f(x_1,x_2,\dots,x_n)] &=  \int f(x_1,x_2,\dots,x_n) P(x_1,x_2,\dots,x_n) dx_1 dx_2 \dots dx_n \\
E[f(x_1,x_2,\dots,x_n)] &=  \sum_{x_1} \sum_{x_2} \dots \sum_{x_n} f(x_1,x_2,\dots,x_n) P(x_1,x_2,\dots,x_n)
\end{align*}

The correlation (dependency) between parameters $x$ and $y$ is described by covariance:
\begin{equation*}
cov(x,y) = E[(x-\mu_x)(y-\mu_y)] = \int (x-\mu_x)(y-\mu_y)P(x,y)dxdy
\end{equation*}
where $P(x,y)$ is a 2-dimensional distribution of variables \enquote{x} and \enquote{y}. It is also convenient to introduce correlation coefficient $\rho$ ($\rho \in [-1; 1]$):
\begin{equation*}
\rho = \frac{cov(x,y)}{\sigma_x \sigma_y}
\end{equation*}
The value of $\rho$ gives information about the dependency between parameters. If $|\rho| = 1$, then it is a case of perfect linear correlation. If $\rho = 0$, variables are independent.

\subsection{Types of distributions: Binomial, Poisson, Gaussian} 
\textbf{The Binomial distribution} describes processes in which outcome of a trial is dichotomous (\textit{yes} or \textit{no}, \textit{head} or \textit{tail} and etc.). The probability of $r$ positive (or negative) outcomes in $N$ trials is calculated in the following way:
\begin{equation*}
P(r) = \frac{N!}{r! (N-r)!}p^r (1-p)^{N-r}
\end{equation*}
where p is the probability of success (or failure) of one single trial. Examples of binomial distribution can be found in Fig.~\ref{fig:Binom}. It is easy to see that binomial distribution is discrete. Theoretical mean value $\mu$ equals to:
\begin{equation} \label{binom_mean_val}
\mu = \sum_r r P(r) = Np
\end{equation}
The variance of the distribution equals to:
\begin{equation*}
\sigma^2 = \sum_r (r-\mu)^2P(r) = Np(1-p)
\end{equation*}
Binomial distribution can be approximated with Poisson or Gaussian distributions, depending on the parameters. 
\begin{figure}[!h]
\begin{center}
\includegraphics*[scale = 0.5]{Figures/Binom}
\caption{\label{fig:Binom} Examples of binomial distribution with different parameter \enquote{$p$}. Details can be found in the legend.}
\end{center}
\end{figure}
\newpage
\textbf{The Poisson distribution} rises as a limit case of Binomial distribution, when the following conditions are satisfied:
\begin{enumerate}
\item Events are independent within any time interval.
\item Probability of one event $p \rightarrow 0$, while the amount of trials $N \rightarrow \infty$. 
\item Mean value $\mu = Np$ stays finite.
\end{enumerate}
Probability to observe $r$ events under the conditions listed above is calculated in the following way:
\begin{equation*}
P(r) = \frac{\mu^r e^{-\mu}}{r!}
\end{equation*}
Poisson distribution has many examples in every day life, for example, the radioactive decay of $^{238}$U isotope. This nucleus is an alpha-decayer with half-life $T_{1/2} \approx 4.5 \times 10^9$ years. From laws of nuclear physics, it is known that nuclei act independently from each other during radioactive decay. The probability of one uranium nucleus to decay in one second is $\lambda = \ln2 /T_{1/2} \approx 5 \times 10^{-18}$ s$^{-1}$. As one can see, probability of one single event is extremely small. But, suppose that we have 1 g of $^{238}$U, which contains $N \approx 2.5 \times 10^{21}$ isotopes, the total number of decay events in one second can be large and can be approximately described by a binomial distribution with $p = 5 \times 10^{-18}$ s$^{-1}$ and $N = 2.5 \times 10^{21}$. By equation \ref{binom_mean_val}, the mean value of the total number of decay events in one second is $\mu = Np \approx 12.5 \times 10^3$ decays/s $ = 12.5 $ decays/ms. If we want a distribution function of the total number of decay events in one millisecond, the Poisson distribution perfectly describes this situation. \par
\begin{figure}[!h]
\begin{center}
\includegraphics*[scale = 0.4]{Figures/Poisson}
\caption{\label{fig:Poisson} Examples of Poisson distribution with different parameter $\mu$. Details can be found in the legend.}
\end{center}
\end{figure}
Poisson distribution is discrete and has only one parameter $\mu$, which stands for the mean value. It can be shown, that variance of the distributions equals to:
\begin{equation*}
\sigma^2 = \mu 
\end{equation*}
On Fig.~\ref{fig:Poisson} one can find several examples of Poisson distribution with different values of $\mu$. As it is seen, with small values of $\mu$, Poisson distribution is asymmetric. But with relatively large values of $\mu$ distributions becomes symmetric and can be approximated by the Gaussian distribution. During such approximation, the discrete nature of Poisson distribution can be neglected.  \par
\textbf{The Gaussian or normal distribution} is one of the most important distributions in statistics and for data analysis in particular. As it was mentioned above, binomial and Poisson distributions can be approximated by Gaussian distribution. Also, it is known from central limit theorem (CLT), that if a set of independent random variables (even with distributions different from normal) are added, their sum will tend to normal distribution. On practice, it means that many processes can be approximated with Gaussian distribution. For example, instrumental errors are generally distributed normally.
The Gaussian distribution is a symmetric, continuous distribution, with density given by:
\begin{equation*}
P(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \Bigg[ - \frac{(x-\mu)^2}{2\sigma^2} \Bigg]
\end{equation*}
It is dependent of two parameters: mean value $\mu$ and standard deviation $\sigma$. Example of Gaussian distribution can be found in Fig.~\ref{fig:Gaussian}. Parameter $\sigma$ characterises the width of Gaussian distribution. It is also common to use another parameter to describe the width, the full width of half maximum (FWHM). It can be shown, that:
\begin{equation*}
\text{FWHM} = 2 \sqrt{2\ln 2}\sigma \approx 2.35 \sigma
\end{equation*}
\begin{figure}[!t]
\begin{center}
\includegraphics*[scale = 0.4]{Figures/Gauss}
\caption{\label{fig:Gaussian} Examples of Gaussian distribution with different parameters $\sigma$. Details can be found in the legend.}
\end{center}
\end{figure}
\begin{figure}[!t]
\begin{center}
\includegraphics*[scale = 0.3]{Figures/Gauss_intervals}
\caption{\label{fig:Gauss_intervals} Area under Gaussian distribution, contained between different intervals. Figure is taken from \cite{stat_2}.}
\end{center}
\end{figure}
Another important feature of normal distribution is the area under it, depending on the intervals. Example can be found in Fig.~\ref{fig:Gauss_intervals}. The area in the interval $[\mu-\sigma;\mu+\sigma]$ equals to $\approx 68\%$. For the interval within $\pm 2\sigma$ and $\pm 3\sigma$, the areas are $95.5\%$ and $99.7\%$ respectively. This result is very important for data representation and means that if one will present experimental results within only 1$\sigma$, then there is approximately a probability of 1/3 that the true value will be outside the region. If results are presented with 2$\sigma$ interval, this probability is less than 5\%.



\section{Statistical methods in experimental physics} \label{methods}

In general case, he estimation of the population mean of a sample $x \in \{ x_1, x_2, x_3, \dots , x_n \}$ is calculated in the following way:
\begin{equation} \label{av_value}
\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} {x_i}
\end{equation}
The estimation of the population standard deviation $s$ is given by the sample standard deviation:
\begin{equation} \label{sample_standard_dev}
s = \sqrt{\frac{1}{(n-1)} \sum_{i = 1}^{n} {(x_i - \bar{x})^2}}
\end{equation}

The estimated standard deviation of $\bar{x}$ is given by:
\begin{equation} \label{standart_dev}
\sigma_{\bar{x}} = \frac{s}{ \sqrt{n} }
\end{equation}

The estimated mean $\bar{x}$ can be described by the normal distribution with the mean $\bar{x}$ and standard deviation $s / \sqrt{n}$, if the sample size is large, typically $n>30$.
Standard deviation $\sigma_{\bar{x}}$ has a meaning of a statistical error and gives a probability of 66$\%$ that the population mean $\mu$ will be found in the region $\bar{x} - \sigma_{\bar{x}} \le x \le \bar{x} + \sigma_{\bar{x}}$. In physics, confidence interval is usually within $2 \sigma$ with the confidence level $\alpha = 95 \%$
\begin{equation} \label{confidence_interval_large}
\bar{x} - 2 \sigma_{\bar{x}} \le x \le \bar{x} + 2 \sigma_{\bar{x}}
\end{equation}

If the sample size is small, $n<30$, we need to use the student's $t$ coefficient $t_{\alpha,n}$.
Its value depends on the sample size n and the confidence level $\alpha$, and their values can be found from the table \ref{table:student_t_value} with the confidence level $\alpha = 95 \%$. The confidence interval with confidence level $\alpha = 95 \%$ is given by
\begin{equation*}
\bar{x} - t_{0.95, n} \sigma_{\bar{x}} \le x \le \bar{x} + t_{0.95, n} \sigma_{\bar{x}}
\end{equation*}
When $n>30$, $t_{0.95, n}$ is reduced to 2 and the confidence interval is reduced to \ref{confidence_interval_large}.


\begin{table}[!h]
\begin{center}
\caption{\label{table:student_t_value} Values of student's $t$ coefficient for confidence level $\alpha$ = 95$\%$.}
\begin{tabular}{l l l l l l l l l}
\hline
n & 2 & 3 & 4 & 5 & 6 & 7-8 & 9-10 & 30-$\infty$  \\
$t_{0.95, n}$ &12.7 & 4.3 & 3.2 & 2.8 & 2.6 & 2.4 & 2.3 & 2 \\
\hline
\end{tabular}
\end{center}
\end{table}

During the experiment, every quantity contains two types of error: statistical and systematical. Final result for value $x$ is written in the following way:
\begin{equation*}
x = \bar{x} \pm \sqrt{\sigma_{stat}^2+\sigma^2_{syst}}
\end{equation*}
 
\subsection{Propagation of Errors}
This section covers a method to estimate the total error $\sigma_{f}$ of a function $f=f(x,y,z)$, when individual error of variables x, y and z are known, namely $\sigma_x$, $\sigma_y$ and $\sigma_z$. It is considered that variables x, y and z are independent. It can be shown that $\sigma_{f}$ can be expressed in the following way:
\begin{equation} \label{propagation_formula}
\sigma_{f} = \sqrt{\Big ( \frac{\partial f}{\partial x} \sigma_x \Big )^2+\Big ( \frac{\partial f}{\partial y} \sigma_y \Big )^2+\Big ( \frac{\partial f}{\partial z} \sigma_z \Big )^2}
\end{equation}
In more general case, if variables x, y and z are dependent, the covariances between variables must be taken into account. Additional information on this case can be found in \cite{stat_1, stat_2, stat_3}.

For example, if $f(x,y,z) = x+y+z$, their partial derivatives are 1. The total error is
\begin{equation*}
\sigma_{f} = \sqrt{( \sigma_x )^2 + ( \sigma_y )^2+ ( \sigma_z )^2}
\end{equation*}
 This means that if $f$ is the sum of x, y and z, the resulting error $\sigma_{f}$ is the square root of the sum of squares of individual errors.
 
Consider the case that $f(x,y,z) = xyz$. The total error is
\begin{align*}
\sigma_{f} &= \sqrt{( yz \sigma_x )^2 + ( xz \sigma_y )^2+ ( yz \sigma_z )^2} \\
\frac{ \sigma_{f} }{xyz} &= \sqrt{ \Big( \frac{\sigma_x}{x} \Big)^2 + \Big( \frac{\sigma_y}{y} \Big)^2 + \Big( \frac{\sigma_z}{z} \Big)^2} \\
\frac{ \sigma_{f} }{f} &= \sqrt{ \Big( \frac{\sigma_x}{x} \Big)^2 + \Big( \frac{\sigma_y}{y} \Big)^2 + \Big( \frac{\sigma_z}{z} \Big)^2}
\end{align*}
This means that if $f$ is the product of x, y and z, the resulting relative error $\sigma_{f}/f$ is the square root of the sum of squares of individual relative errors.

\section{Fitting methods}
A fitting method is a process of constructing a curve, or mathematical function, that best fits to the experimental data.
The form of the theoretical function needs to be known before the fitting process.
In general, the form of the theoretical function is a function of one varible $x$ and some parameters $(\theta_1,\theta_2,\dots,\theta_m)$.
\begin{equation*}
y = f(x;\theta_1,\theta_2,\dots,\theta_m)
\end{equation*}
The fitting process is to find the best values of the parameters, which best describe the experimental data.
Also, the errors of the estimation of the best-fit values need to be calculated.
In this section, we will introduce two fitting methods: the maximum likelihood method and the $\chi^2$ method.
In the maximum likelihood method, the experimental data is a sample of n independent observations $\{ x_1, x_2, \dots , x_n \}$, which are from a theoretical probability density function $y = f(x;\theta_1,\theta_2,\dots,\theta_m)$.
In the $\chi^2$ method, the experimental data is a set of points $(x_i,y_i)$, which are related by a theoretical function $y = f(x;\theta_1,\theta_2,\dots,\theta_m)$ .

\subsection{Maximum likelihood method \cite{stat_2}}
Suppose that we have a sample of n independent observations $\{ x_1, x_2, \dots , x_n \}$, which are from a theoretical probability density function $y = f(x;\theta_1,\theta_2,\dots,\theta_m)$, where $(\theta_1,\theta_2,\dots,\theta_m)$ are the true values of the parameters of the theoretical probability density function.
The probability of observing a specific sample of n independent observations $\{ x_1, x_2, \dots , x_n \}$ is called the likelihood function $L$ and is calculated by the product of the individual probability.
\begin{equation*}
L(\theta_1,\theta_2,\dots,\theta_m;x_1,x_2,\dots,x_n) = \prod_{i = 1}^{n} f(x_i;\theta_1,\theta_2,\dots,\theta_m)
\end{equation*}
The principle for finding the best-fit values of the parameters is to maximize the likelihood function $L$.
Every best-fit values of the parameters need to satisfy the following maximization condition:
\begin{equation}  \label{likelihood_maximization}
\frac{\partial L}{\partial \theta_j} = 0 \qquad \text{for } 1 \leq  j \leq m
\end{equation}
Depending on the form of $L$, it may also be easier to maximize the logarithm of $L$, rather than $L$ itself.
\begin{equation*}
\frac{\partial ( \ln L )}{\partial \theta_j} = 0 \qquad \text{for } 1 \leq  j \leq m
\end{equation*}
which yields the same results as \ref{likelihood_maximization}.
By solving the equations, the solution $(\hat{\theta}_1,\hat{\theta}_2,\dots,\hat{\theta}_m)$ are called the maximum likelihood estimator for the parameter $\theta$.
The caret over the parameters is to distinguish the estimated value from the true value.

It should be realized now that the estimator $\hat{\theta}_j$ is also a random variable, since it is a function of $x_i$.
\begin{equation*}
\hat{\theta}_j = \hat{\theta}_j(x_1,x_2,\dots,x_n)
\end{equation*}
If a second sample is taken, the estimator $\hat{\theta}_j$ will have a different value.
The estimator $\hat{\theta}_j$ is thus also described by a probability distribution.
This leads us to the second half of the estimation problem: What is the error of the estimators $\hat{\theta}_j$?
This is given by the standard deviation of the estimator distribution.
First, we need to calculate the mean of the estimator distribution, which is given by the equation \ref{first_moment}.
\begin{align*}
\mu_{\hat{\theta}_j}
&= E[\hat{\theta}_j(x_1,x_2,\dots,x_n)] \\
&= \int \hat{\theta}_j(x_1,x_2,\dots,x_n) P(x_1,x_2,\dots,x_n) dx_1 dx_2 \dots dx_n \\
&= \int \hat{\theta}_j(x_1,x_2,\dots,x_n) \Big[ \prod_{i = 1}^{n} f(x_i;\theta_1,\theta_2,\dots,\theta_m) \Big] dx_1 dx_2 \dots dx_n \\
&= \int \hat{\theta}_j(x_1,x_2,\dots,x_n) L(\theta_1,\theta_2,\dots,\theta_m;x_1,x_2,\dots,x_n) dx_1 dx_2 \dots dx_n
\end{align*}
After the integration, it only depends on the true values of parameters $(\theta_1,\theta_2,\dots,\theta_m)$.
A estimator with the property that $\mu_{\hat{\theta}_j} = \theta_j$ is called an unbiased estimator.
Otherwise, it is called a biased estimator.
The variance of the estimator distribution can be calculated by the equation \ref{second_moment}.
\begin{align*}
\sigma_{\hat{\theta}_j}^2
&= E[(\hat{\theta}_j - \mu_{\hat{\theta}_j} )^2] \\
&= \int (\hat{\theta}_j - \mu_{\hat{\theta}_j} )^2 P(x_1,x_2,\dots,x_n) dx_1 dx_2 \dots dx_n \\
&= \int (\hat{\theta}_j - \mu_{\hat{\theta}_j} )^2 \Big[ \prod_{i = 1}^{n} f(x_i;\theta_1,\theta_2,\dots,\theta_m) \Big] dx_1 dx_2 \dots dx_n
\end{align*}

\subsubsection{Estimator of Poisson distribution}
Suppose we have n independent measurements of samples $\{ x_1, x_2, \dots , x_n \}$ from a Poisson distribution with the mean $\lambda$. The likelihood function is
\begin{align*}
L(\lambda;x_1,x_2,\dots,x_n) &= \prod_{i = 1}^{n} \frac{\lambda^{x_i}}{x_i !} \exp(-\lambda) \\
&= \exp(-n \lambda) \prod_{i = 1}^{n} \frac{\lambda^{x_i}}{x_i !} \\
\ln L &= -n \lambda + \sum_{i = 1}^{n} x_i \ln(\lambda) - \sum_{i = 1}^{n} \ln(x_i !) \\
&= -n \lambda + \ln(\lambda) \sum_{i = 1}^{n} x_i - \sum_{i = 1}^{n} \ln(x_i !)
\end{align*}
Differentiating and setting the result to zero, we then find
\begin{align*}
\frac{\partial ( \ln L )}{\partial \lambda} = -n + \frac{1}{\lambda} \sum_{i = 1}^{n} x_i &= 0 \\
\hat{\lambda} &= \frac{1}{n} \sum_{i = 1}^{n} x_i \\
\hat{\lambda} &= \bar{x}
\end{align*}
The estimator of $\hat{\lambda}$ is just the sample mean $\bar{x}$.

To calculate the error of the estimator, we need to first calculate the mean of estimator $\mu_{\hat{\lambda}}$. (or see appendix \ref{proof_mean})
\begin{align*}
\mu_{\hat{\lambda}}
&= E[\hat{\lambda}] \\
&= E[\bar{x}] \\
&= E \Bigg[ \frac{1}{n} \sum_{i=1}^n x_i \Bigg] \\
&= \frac{1}{n} E \Bigg[ \sum_{i=1}^n x_i \Bigg] \\
&= \frac{1}{n} \sum_{i=1}^n E[x_i] \\
&= \frac{1}{n} \sum_{i=1}^n \lambda \\
&= \frac{1}{n} (n \lambda) \\
&= \lambda
\end{align*}
Therefore, the estimator $\hat{\lambda}$ is an unbiased estimator.
The reader may have noticed that the derivation of the result
\begin{equation} \label{mean_of_sample_mean}
E[\bar{x}]  = \mu
\end{equation}
is independent of the probability density function (Poisson distribution in this case, $\mu=\lambda$), and hence this is a general result.

The variance is
\begin{align*}
\sigma_{\hat{\lambda}}^2
&= E[(\hat{\lambda} - \mu_{\hat{\lambda}} )^2] \\
&= E[(\hat{\lambda} - \lambda )^2] \\
&= E[(\bar{x} - \lambda )^2] \\
&= E\Bigg [ \Bigg( \Bigg( \frac{1}{n} \sum_{i=1}^n x_i \Bigg) - \lambda \Bigg)^2 \Bigg] \\
&= E\Bigg [ \Bigg( \frac{1}{n} \sum_{i=1}^n (x_i - \lambda) \Bigg)^2 \Bigg] \\
&= \frac{1}{n^2} E\Bigg [ \Bigg( \sum_{i=1}^n (x_i - \lambda) \Bigg)^2 \Bigg] \\
&= \frac{1}{n^2} E\Bigg [ \Bigg( \sum_{i=1}^n (x_i - \lambda) \Bigg) \Bigg( \sum_{j=1}^n (x_j - \lambda) \Bigg) \Bigg] \\
&= \frac{1}{n^2} E\Bigg [ \sum_{i=1}^n \sum_{j=1}^n (x_i - \lambda) (x_j - \lambda) \Bigg] \\
&= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n E[ (x_i - \lambda) (x_j - \lambda) ] \\
\end{align*}
The last term is just a covariance. Because $\{ x_1, x_2, \dots , x_n \}$ are independent, $E[f(x_1)g(x_2)] = E[f(x_1)] E[g(x_2)]$.
\begin{align}
\sum_{i=1}^n \sum_{j=1}^n E [ (x_i - \lambda) (x_j - \lambda) ]
&= \sum_{i=1}^n    E[ (x_i - \lambda)^2 ]
+  \sum_{i \neq j} E[ (x_i - \lambda) (x_j - \lambda) ] \\
&= \sum_{i=1}^n \sigma^2
+  \sum_{i \neq j} E[x_i - \lambda] E[x_j - \lambda] \\
&= (n \sigma^2)
+  \sum_{i \neq j} (E[x_i] - E[\lambda])( E[x_j] - E[\lambda]) \\
&= n \sigma^2
+  \sum_{i \neq j} (\lambda - \lambda)(\lambda - \lambda) \\
&= n \sigma^2 \label{expectation_covariance}
\end{align}
The variance then is
\begin{align*}
\sigma_{\hat{\lambda}}^2
&= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n E[ (x_i - \lambda) (x_j - \lambda) ] \\
&= \frac{1}{n^2} (n \sigma^2) \\
&= \frac{\sigma^2}{n} \\
\sigma_{\hat{\lambda}} &= \frac{\sigma}{\sqrt{n}} = \sqrt{\frac{\lambda}{n}}
\end{align*}
The theoretical mean $\lambda$ can be estimated by $\hat{\lambda}$.
\begin{align*}
\sigma_{\hat{\lambda}}
&= \sqrt{\frac{\lambda}{n}} \\
&\approx \sqrt{\frac{\hat{\lambda}}{n}} \\
&= \sqrt{\frac{\bar{x}}{n}}
\end{align*}
The reader may have noticed that the derivation of the result
\begin{equation} \label{error_of_sample_mean}
E[(\bar{x} - \mu )^2] = \frac{\sigma^2}{n}
\end{equation}
is independent of the probability density function (Poisson distribution in this case, $\mu=\lambda$, $\sigma^2=\lambda$), and hence this is a general result. This result also justifies the equation \ref{standart_dev}.

\subsubsection{Estimators of Gaussian distribution}
Suppose we have n independent measurements of samples $\{ x_1, x_2, \dots , x_n \}$ from a Gaussian distribution. The likelihood function is
\begin{align*}
L(\mu, \sigma^2 ;x_1,x_2,\dots,x_n) &= \prod_{i = 1}^{n} \frac{1}{\sigma \sqrt{2\pi}} \exp \Bigg[ - \frac{(x_i-\mu)^2}{2\sigma^2} \Bigg]
\end{align*}
\begin{align*}
\ln L
&= -\frac{n}{2} \ln(2 \pi \sigma^2) - \frac{1}{2} \sum_{i = 1}^{n} \frac{(x_i - \mu)^2}{\sigma^2}
\end{align*}
Differentiating with respect to $\mu$ and setting the result to zero, we then find
\begin{align*}
\frac{\partial ( \ln L )}{\partial \mu} =
\sum_{i = 1}^{n} \frac{x_i - \mu}{\sigma^2} &= 0 \\
\sum_{i = 1}^{n} (x_i - \mu) &= 0 \\
\Bigg(\sum_{i = 1}^{n} x_i \Bigg) - n \mu &= 0 \\
\hat{\mu} &= \frac{1}{n} \sum_{i = 1}^{n} x_i \\
\hat{\mu} &= \bar{x}
\end{align*}
By equation \ref{mean_of_sample_mean} and \ref{error_of_sample_mean}, the error of $\hat{\mu}$ is
\begin{align}
\sigma_{\hat{\mu}}^2
&= E[(\hat{\mu} - \mu_{\hat{\mu}} )^2] \\
&= E[(\bar{x} - \mu )^2] \\
&= \frac{\sigma^2}{n} \\
\sigma_{\hat{\mu}} &= \frac{\sigma}{\sqrt{n}} \\
&\approx \frac{\hat{\sigma}}{\sqrt{n}} \label{error_of_estimator_mu}
\end{align}
The theoretical standard derivation $\sigma$ is unknown, but can be estimated by $\hat{\sigma}$.
Differentiating $\ln L$ with respect to $\sigma^2$ and setting the result to zero, $\hat{\sigma}$ can be found.
\begin{align*}
\frac{\partial ( \ln L )}{\partial \sigma^2} =
-\frac{n}{2 \sigma^2} + \frac{1}{2} \sum_{i = 1}^{n} \frac{(x_i - \mu)^2}{\sigma^2} \frac{1}{\sigma^2} &= 0 \\
-n + \frac{1}{\sigma^2} \sum_{i = 1}^{n} (x_i - \mu)^2 &= 0 \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i = 1}^{n} (x_i - \mu)^2 \\
&\approx \frac{1}{n} \sum_{i = 1}^{n} (x_i - \hat{\mu})^2 \\
&= \frac{1}{n} \sum_{i = 1}^{n} (x_i - \bar{x})^2
\end{align*}
The mean of the estimator $\hat{\sigma}^2$ is
\begin{align*}
\mu_{\hat{\sigma}^2}
&= E \Bigg[ \frac{1}{n} \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg] \\
&= \frac{1}{n} \sum_{i = 1}^{n} E [(x_i - \bar{x})^2] \\
&= \frac{1}{n} \sum_{i = 1}^{n} E [((x_i - \mu) - (\bar{x} - \mu))^2] \\
&= \frac{1}{n} \sum_{i = 1}^{n} E [(x_i - \mu)^2 + (\bar{x} - \mu)^2 - 2 (x_i - \mu)(\bar{x} - \mu)] \\
&= \frac{1}{n} \sum_{i = 1}^{n} E [(x_i - \mu)^2]
+ \frac{1}{n} \sum_{i = 1}^{n} E [(\bar{x} - \mu)^2]
- \frac{2}{n} \sum_{i = 1}^{n} E [(x_i - \mu)(\bar{x} - \mu)] \\
&= \frac{1}{n} \sum_{i = 1}^{n} \sigma^2
+ \frac{1}{n} \sum_{i = 1}^{n} \frac{\sigma^2}{n}
- \frac{2}{n} \sum_{i = 1}^{n} E [(x_i - \mu)(\bar{x} - \mu)] \\
\end{align*}
\begin{align*}
\sum_{i = 1}^{n} E [(x_i - \mu)(\bar{x} - \mu)]
&= \sum_{i = 1}^{n} E \Bigg[ (x_i - \mu) \Bigg( \Bigg( \frac{1}{n} \sum_{j = 1}^{n} x_j \Bigg) - \mu \Bigg) \Bigg] \\
&= \sum_{i = 1}^{n} E \Bigg[ (x_i - \mu) \frac{1}{n} \sum_{j = 1}^{n} (x_j - \mu) \Bigg] \\
&= \frac{1}{n}\sum_{i = 1}^{n} \sum_{j = 1}^{n} E[ (x_i - \mu) (x_j - \mu) ] \\
&= \frac{1}{n} (n \sigma^2) \\
&= \sigma^2
\end{align*}
The last step is by equation \ref{expectation_covariance}. The mean of the estimator $\hat{\sigma}^2$ is
\begin{align*}
\mu_{\hat{\sigma}^2}
&= \frac{1}{n} \sum_{i = 1}^{n} \sigma^2
+  \frac{1}{n} \sum_{i = 1}^{n} \frac{\sigma^2}{n}
-  \frac{2}{n} \sigma^2 \\
&= \sigma^2
+  \frac{\sigma^2}{n}
-  \frac{2\sigma^2}{n} \\
&= \sigma^2 - \frac{\sigma^2}{n} \\
&= \frac{(n-1)\sigma^2}{n} \\
\end{align*}
Hence, the estimator $\hat{\sigma}^2$ is a biased estimator. Sometimes, it is better to use the following equation to estimate $\sigma$.
\begin{align*}
\hat{\sigma}^2 &\approx \mu_{\hat{\sigma}^2} \\
\frac{1}{n} \sum_{i = 1}^{n} (x_i - \bar{x})^2 &\approx \frac{(n-1)\sigma^2}{n} \\
\sigma^2 &\approx \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2 \\
\end{align*}
This result also justifies the equation \ref{sample_standard_dev}.
We can re-define the estimator $\hat{\sigma}^2$.
\begin{equation} \label{estimator_sigma_redefinition}
\hat{\sigma}^2 = \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2
\end{equation}
Hence, the estimator $\hat{\sigma}^2$ defined by equation \ref{estimator_sigma_redefinition} is an unbiased estimator.
\begin{equation*}
\mu_{\hat{\sigma^2}} = \sigma^2
\end{equation*}
The error of $\hat{\mu}$ in equation \ref{error_of_estimator_mu} can then be estimated by
\begin{align*}
\sigma_{\hat{\mu}}
&\approx \frac{\hat{\sigma}}{\sqrt{n}} \\
&= \frac{1}{\sqrt{n}} \sqrt{ \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2 }
\end{align*}
Finally, we need to calculate the error of the estimator $\hat{\sigma}^2$ defined by equation \ref{estimator_sigma_redefinition}. It is calculated in the appendix \ref{proof_variance}.
\begin{align*}
\sigma_{\hat{\sigma^2}}^2 &= \frac{2\sigma^4}{n-1} \\
\sigma_{\hat{\sigma^2}} &= \sqrt{\frac{2}{n-1}} \sigma^2 \\
\sigma_{\hat{\sigma^2}} &= \sqrt{\frac{2}{n-1}} \hat{\sigma^2}
\end{align*}
By equation \ref{propagation_formula}, the error of the estimator $\hat{\sigma}$ is
\begin{align*}
\sigma_{\hat{\sigma^2}} &= \frac{d(\hat{\sigma^2})}{d\hat{\sigma}} \sigma_{\hat{\sigma}} \\
\sigma_{\hat{\sigma^2}} &= 2 \hat{\sigma} \sigma_{\hat{\sigma}} \\
\sigma_{\hat{\sigma}} &= \frac{\sigma_{\hat{\sigma^2}}} {2 \hat{\sigma}} \\
&= \frac{1}{2\hat{\sigma}} \sqrt{\frac{2}{n-1}} \hat{\sigma^2} \\
&= \frac{\hat{\sigma}}{\sqrt{2(n-1)}} \\
&= \frac{1}{\sqrt{2(n-1)}} \sqrt{ \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2 } \\
\end{align*}

\subsection{$\chi^2$ method}
The $\chi^2$ method is useful for testing how good the fit is.
$\chi^2$ analysis \cite{stat_2, BAKER1984437, reduced_chi_squared} is based on minimizing the difference between experimental data and the fitting function. Suppose that we have a series of experimental data $(x_i,y_i)$ with the error of $y_i$ denoted by $\sigma_i$, and we use a fitting function $y = f(x;\theta_1,\theta_2,\dots,\theta_m)$ to fit the experimental data $y_i$ by varing different values of the m parameters $\theta_j$. The deviation bewteen the experimental data and the fitting curve can be measured by the following quantity $\chi^2$:
\begin{equation} \label{chi_square_formula}
\chi^2 = \sum_{i = 1}^{n} \Big ( \frac{y_i - f(x_i;\theta_1,\theta_2,\dots,\theta_m)}{\sigma_i} \Big )^2
\end{equation}
where summation is taken over all experimental points.
By finding the minimum value of $\chi^2$ by trying all possible values of the m parameters $\theta_j$, the best values of the parameters can be found and hence the best-fit curve can be found.
Every best-fit values of the parameters need to satisfy the following minimization condition:
\begin{equation} \label{chi_minimisation_condition}
\frac{\partial \chi^2}{\partial \theta_j} = 0 \qquad \text{for } 1 \leq  j \leq m
\end{equation}

\subsubsection{Linear fit}
We can use the linear fit as an example for using the $\chi^2$ method.
Suppose the fitting function is $y = f(x;a,b) = ax + b$, with the parameters a and b.
By the equation \ref{chi_square_formula}, the $\chi^2$ is
\begin{equation*}
\chi^2 = \sum_{i = 1}^{n} \Big ( \frac{y_i - a x_i - b}{\sigma_i} \Big )^2
\end{equation*}
To find the best-fit values of a and b, by the equation \ref{chi_minimisation_condition}, we need to solve the following system of equations:
\begin{equation} \label{equation_a}
\frac{\partial \chi^2}{\partial a} = -2 \sum_{i = 1}^{n} \frac{ (y_i - a x_i - b) x_i }{\sigma_i^2} = 0
\end{equation}
\begin{equation} \label{equation_b}
\frac{\partial \chi^2}{\partial b} = -2 \sum_{i = 1}^{n} \frac{ (y_i - a x_i - b) }{\sigma_i^2} = 0
\end{equation}
To simplify the notation, we can define the following sums
\begin{equation*}
S_0 = \sum_{i = 1}^{n} \frac{1}{\sigma_i^2} \qquad
S_x = \sum_{i = 1}^{n} \frac{x_i}{\sigma_i^2} \qquad
S_y = \sum_{i = 1}^{n} \frac{y_i}{\sigma_i^2}
\end{equation*}
\begin{equation*}
S_{xx} = \sum_{i = 1}^{n} \frac{x_i^2}{\sigma_i^2} \qquad
S_{yy} = \sum_{i = 1}^{n} \frac{y_i^2}{\sigma_i^2} \qquad
S_{xy} = \sum_{i = 1}^{n} \frac{x_i y_i}{\sigma_i^2}
\end{equation*}
Equations \ref{equation_a} and \ref{equation_b} can be simplified to
\begin{equation*}
-2(S_{xy} - a S_{xx} - b S_x) = 0
\end{equation*}
\begin{equation*}
-2(S_y - a S_x - b S_0) = 0
\end{equation*}
By solving the liner equations, the best-fit values of a and b are
\begin{equation*}
a = \frac{S_0 S_{xy} - S_x S_y}{S_0 S_{xx} - S_x^2}
\end{equation*}
\begin{equation*}
b = \frac{S_y S_{xx} - S_x S_{xy}}{S_0 S_{xx} - S_x^2}
\end{equation*}
The errors of a and b are given by the further reading \cite{stat_2}.
\begin{equation*}
\sigma_a^2 = \frac{S_0}{S_0 S_{xx} - S_x^2}
\end{equation*}
\begin{equation*}
\sigma_b^2 = \frac{S_{xx}}{S_0 S_{xx} - S_x^2}
\end{equation*}
To measure the quality of the fit, we can divide the $\chi^2$ by the degree of freedom $n-2$.
\begin{equation} \label{reduced_chi_square}
\frac{\chi^2}{n-2}
\end{equation}
The fit is good if the expression \ref{reduced_chi_square} is close to 1.

\subsubsection{Analysis of data in case of large statistics}
The technique, discussed at the beginning of section \ref{methods}, can always be used .
But in the case of large statistics of experimental data, we can analyse the data by fitting a histogram by a normal distribution, because the histogram is reliable only if the number of experimental data is large.
We can do many sets of experiments.
And, for each experiments, we collect a fixed number of the experimental data and calculate its means.
According to the central limit theorem, the mean of a experiment can be approximately described by a normal distribution.

In figure~\ref{fig:radius_distribution}, it shows the examples of the histograms of means of a radius for a particle $r$ and square displacement of a Brownian particle $\langle x^2 \rangle$.
The histogram allows us to measure the mean value $\bar{x}$ and its standard deviation $\sigma_{\bar{x}}$, described in equation \ref{av_value} and \ref{standart_dev}, by fitting the histogram by a normal distribution.

\begin{figure}[!h]
\begin{center}
\includegraphics*[scale = 0.25]{Figures/two_plots}
\caption{\label{fig:radius_distribution} Example of the distribution of a radius for a particle and square displacement of a Brownian particle. Data is generated using Gaussian distribution in both cases.}
\end{center}
\end{figure}

Final result with confidence interval of confidence level $\alpha = 66 \%$ is
\begin{equation*}
x = \bar{x} \pm \sqrt{ (\sigma_{stat})^2 + (\sigma_{syst})^2 }
\end{equation*}
Final result with confidence interval of $\alpha = 95 \%$ is
\begin{equation*}
x = \bar{x} \pm \sqrt{ (2\sigma_{stat})^2 + (\sigma_{syst})^2}
\end{equation*}



\section{Experimental Setup and Procedure}
Experimental equipment consists of the following parts:
\begin{enumerate}
\item Standard micrometer
\item Thin aluminium plate
\end{enumerate}
A student can find a list of \enquote{must-do} tasks below, but students are highly motivated to expand to data analysis further:

\begin{enumerate}
\item Measure the thickness of a thin aluminium plate, using standard micrometer. Obtain data of large (200 points) and low (10 points) statistics. Estimate mean value (\ref{av_value}) and standard deviation (\ref{standart_dev}) for each case. Fit each data set with a distribution function, extract mean value and standard deviation from the fitting. Calculate $\chi^2$ (\ref{chi_square_formula}) for each case and define \enquote{goodness} of the fit. Present your results with 95$\%$ confidence level, taking into account statistical and systematical errors. 
\item Simulate Landau distribution sitting on a background curve (exponential decay). Add statistical fluctuations and error bars (take as $\sqrt{N}$) to data points. Fit the data, estimate \enquote{goodness} of the fit, extract parameters of the distribution and compare them with the simulated values. Present your results with 95$\%$ confidence level, taking into account statistical and systematical errors.
\item Simulate two Gaussian distributions overlapping with each other, and sitting on the background curve (exponential decay). Simulate several cases with Gaussians of the same and different statistics, different widths of Gaussian distributions. Fit the data, estimate \enquote{goodness} of the fit, extract parameters of the distribution and compare them with the simulated values for each case. Present your results with 95$\%$ confidence level, taking into account statistical and systematical errors.
\end{enumerate}

\section{Discussion}
To successfully pass the lab, a student must complete the following tasks:
\begin{enumerate}
\item What is the distribution function of the thickness of aluminium plate? How many parameters does it have? 
\item In what cases does the $\chi^2$ analysis method can be used? Derive $\chi^2$ analysis method from maximum likelihood method.
\item Other questions
\end{enumerate}

\section{Appendix: Proof that the mean of the sample mean is the population mean} \label{proof_mean}
Prove by the first principle (discrete variable):
\begin{align*}
\mu_{\bar{x}}
&= \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} \bar{x} \prod_{i = 1}^{n} P(x_i) \\
&= \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} \frac{1}{n} ( x_1 + x_2 + \dots + x_n) \Big[ P(x_1) P(x_2) \dots P(x_n) \Big] \\
&= \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} x_1 P(x_1) P(x_2) \dots P(x_n) \\
&+ \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} x_2 P(x_1) P(x_2) \dots P(x_n) \\
& \qquad \vdots \\
&+ \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} x_n P(x_1) P(x_2) \dots P(x_n) \\
&= \frac{1}{n} \sum_{x_2 = 0}^{\infty} \sum_{x_3 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} P(x_2) P(x_3) \dots P(x_n) \sum_{x_1 = 0}^{\infty} x_1 P(x_1) \\
&+ \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_3 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} P(x_1) P(x_3) \dots P(x_n) \sum_{x_2 = 0}^{\infty} x_2 P(x_2) \\
& \qquad \vdots \\
&+ \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_{n-1} = 0}^{\infty} P(x_1) P(x_2) \dots P(x_{n-1}) \sum_{x_n = 0}^{\infty} x_n P(x_n) \\
&= \frac{1}{n} \sum_{x_2 = 0}^{\infty} \sum_{x_3 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} P(x_2) P(x_3) \dots P(x_n) \mu \\
&+ \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_3 = 0}^{\infty} \dots \sum_{x_n = 0}^{\infty} P(x_1) P(x_3) \dots P(x_n) \mu \\
& \qquad \vdots \\
&+ \frac{1}{n} \sum_{x_1 = 0}^{\infty} \sum_{x_2 = 0}^{\infty} \dots \sum_{x_{n-1} = 0}^{\infty} P(x_1) P(x_2) \dots P(x_{n-1}) \mu \\
&= \frac{\mu}{n} \Bigg( \sum_{x_2 = 0}^{\infty} P(x_2) \Bigg) \Bigg( \sum_{x_3 = 0}^{\infty} P(x_3) \Bigg) \dots \Bigg( \sum_{x_n = 0}^{\infty} P(x_n) \Bigg) \\
&+ \frac{\mu}{n} \Bigg( \sum_{x_1 = 0}^{\infty} P(x_1) \Bigg) \Bigg( \sum_{x_3 = 0}^{\infty} P(x_3) \Bigg) \dots \Bigg( \sum_{x_n = 0}^{\infty} P(x_n) \Bigg) \\
& \qquad \vdots \\
&+ \frac{\mu}{n} \Bigg( \sum_{x_1 = 0}^{\infty} P(x_1) \Bigg) \Bigg( \sum_{x_2 = 0}^{\infty} P(x_2) \Bigg) \dots \Bigg( \sum_{x_{n-1} = 0}^{\infty} P(x_{n-1}) \Bigg)
\end{align*}

\begin{align*}
\mu_{\bar{x}}
&= \frac{\mu}{n} (1) (1) \dots (1) \\
&+ \frac{\mu}{n} (1) (1) \dots (1) \\
& \qquad \vdots \\
&+ \frac{\mu}{n} (1) (1) \dots (1) \\
&= \mu
\end{align*}

\section{Appendix: Prove the variance of $\hat{\sigma^2}$} \label{proof_variance}
\begin{align}
\sigma_{\hat{\sigma^2}}^2
&= E[(\hat{\sigma^2} - \mu_{\hat{\sigma^2}} )^2] \\
&= E \Bigg[ \Bigg( \Bigg( \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg) - \sigma^2 \Bigg)^2 \Bigg] \\
&= E \Bigg[ \Bigg( \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg]
-2 \sigma^2 E \Bigg[ \frac{1}{n-1} \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg] + \sigma^4 \\
&= \frac{1}{(n-1)^2} E \Bigg[ \Bigg( \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg]
-2 \sigma^2 \sigma^2 + \sigma^4 \\
&= \frac{1}{(n-1)^2} E \Bigg[ \Bigg( \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg] - \sigma^4 \label{proof_variance_origin1}
\end{align}
\begin{align}
&E \Bigg[ \Bigg( \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg] \\
&= E \Bigg[ \Bigg( \sum_{i = 1}^{n} \Big[ (x_i - \mu) - (\bar{x} - \mu) \Big]^2 \Bigg)^2 \Bigg] \\
&= E \Bigg[ \Bigg( \sum_{i = 1}^{n} \Big[ (x_i - \mu)^2 + (\bar{x} - \mu)^2 - 2 (x_i - \mu)(\bar{x} - \mu) \Big] \Bigg)^2 \Bigg] \\
&= E \Bigg[ \sum_{i = 1}^{n} \sum_{j = 1}^{n} \Big[ (x_i - \mu)^2 + (\bar{x} - \mu)^2 - 2 (x_i - \mu)(\bar{x} - \mu) \Big] \Big[ (x_j - \mu)^2 + (\bar{x} - \mu)^2 - 2 (x_j - \mu)(\bar{x} - \mu) \Big] \Bigg] \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu)^2]
+  \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(\bar{x} - \mu)^4]
+4 \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu) (x_j - \mu) (\bar{x} - \mu)^2] \\
&+2\sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (\bar{x} - \mu)^2]
-4 \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu) (\bar{x} - \mu)^3]
-4 \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu) (\bar{x} - \mu)] \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu)^2]
+  n^2 E[(\bar{x} - \mu)^4]
+4 \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu) (x_j - \mu) (\bar{x} - \mu)^2] \\
&+2n \sum_{i = 1}^{n} E[(x_i - \mu)^2 (\bar{x} - \mu)^2]
-4n  \sum_{i = 1}^{n} E[(x_i - \mu) (\bar{x} - \mu)^3]
-4 \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu) (\bar{x} - \mu)]  \label{proof_variance_origin2}
\end{align}

Calculate term 2 to term 6 respectively. \\
Term 2
\begin{align}
& E[(\bar{x} - \mu)^4] \\
&=E \Bigg[ \Bigg( \Bigg( \frac{1}{n} \sum_{i=1}^n x_i \Bigg) - \mu \Bigg)^4 \Bigg] \\
&=E \Bigg[ \Bigg( \frac{1}{n} \sum_{i=1}^n (x_i - \mu) \Bigg)^4 \Bigg] \\
&=\frac{1}{n^4} E \Bigg[ \Bigg(\sum_{i=1}^n (x_i - \mu) \Bigg)^4 \Bigg] \\
&=\frac{1}{n^4} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)] \label{proof_variance_term2}
\end{align}
Term 3
\begin{align}
& \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu) (x_j - \mu) (\bar{x} - \mu)^2] \\
&=\sum_{i = 1}^{n} \sum_{j = 1}^{n} E\Bigg[ (x_i - \mu) (x_j - \mu) \Bigg( \Bigg( \frac{1}{n} \sum_{k=1}^n x_k \Bigg) - \mu \Bigg)^2 \Bigg] \\
&=\sum_{i = 1}^{n} \sum_{j = 1}^{n} E\Bigg[ (x_i - \mu) (x_j - \mu) \Bigg( \frac{1}{n} \sum_{k=1}^n (x_k - \mu) \Bigg)^2 \Bigg] \\
&=\frac{1}{n^2} \sum_{i = 1}^{n} \sum_{j = 1}^{n} E\Bigg[ (x_i - \mu) (x_j - \mu) \Bigg( \sum_{k=1}^n (x_k - \mu) \Bigg)^2 \Bigg] \\
&=\frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)]  \label{proof_variance_term3}
\end{align}
Term 4
\begin{align}
& \sum_{i = 1}^{n} E[(x_i - \mu)^2 (\bar{x} - \mu)^2] \\
&=\sum_{i = 1}^{n} E \Bigg[ (x_i - \mu)^2 \Bigg( \Bigg( \frac{1}{n} \sum_{j=1}^n x_j \Bigg) - \mu \Bigg)^2 \Bigg] \\
&=\sum_{i = 1}^{n} E \Bigg[ (x_i - \mu)^2 \Bigg( \frac{1}{n} \sum_{j=1}^n (x_j - \mu) \Bigg)^2 \Bigg] \\
&=\frac{1}{n^2} \sum_{i = 1}^{n} E \Bigg[ (x_i - \mu)^2 \Bigg( \sum_{j=1}^n (x_j - \mu) \Bigg)^2 \Bigg] \\
&=\frac{1}{n^2} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k = 1}^{n} E[ (x_i - \mu)^2 (x_j - \mu) (x_k - \mu) ]  \label{proof_variance_term4}
\end{align}
Term 5
\begin{align}
& \sum_{i = 1}^{n} E[(x_i - \mu) (\bar{x} - \mu)^3] \\
&=\sum_{i = 1}^{n} E \Bigg[(x_i - \mu) \Bigg( \Bigg( \frac{1}{n} \sum_{j=1}^n x_j \Bigg) - \mu \Bigg)^3 \Bigg] \\
&=\sum_{i = 1}^{n} E \Bigg[(x_i - \mu) \Bigg( \frac{1}{n} \sum_{j=1}^n (x_j - \mu) \Bigg)^3 \Bigg] \\
&=\frac{1}{n^3} \sum_{i = 1}^{n} E \Bigg[(x_i - \mu) \Bigg( \sum_{j=1}^n (x_j - \mu) \Bigg)^3 \Bigg] \\
&=\frac{1}{n^3} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)]  \label{proof_variance_term5}
\end{align}
Term 6
\begin{align}
& \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu) (\bar{x} - \mu)] \\
&=\sum_{i = 1}^{n} \sum_{j = 1}^{n} E \Bigg[(x_i - \mu)^2 (x_j - \mu) \Bigg( \Bigg( \frac{1}{n} \sum_{k=1}^n x_k \Bigg) - \mu \Bigg) \Bigg] \\
&=\sum_{i = 1}^{n} \sum_{j = 1}^{n} E \Bigg[(x_i - \mu)^2 (x_j - \mu) \Bigg( \frac{1}{n} \sum_{k=1}^n (x_k - \mu) \Bigg) \Bigg] \\
&=\frac{1}{n} \sum_{i = 1}^{n} \sum_{j = 1}^{n} E \Bigg[(x_i - \mu)^2 (x_j - \mu) \Bigg( \sum_{k=1}^n (x_k - \mu) \Bigg) \Bigg] \\
&=\frac{1}{n} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k=1}^n E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu) ] \label{proof_variance_term6}
\end{align}

Substitute \ref{proof_variance_term2}, \ref{proof_variance_term3}, \ref{proof_variance_term4}, \ref{proof_variance_term5} and \ref{proof_variance_term6} into \ref{proof_variance_origin2}
\begin{align}
&E \Bigg[ \Bigg( \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg] \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu)^2] \\
&+ n^2 \frac{1}{n^4} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)] \\
&+4 \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)] \\
&+2n \frac{1}{n^2} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k = 1}^{n} E[ (x_i - \mu)^2 (x_j - \mu) (x_k - \mu) ] \\
&-4n  \frac{1}{n^3} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)] \\
&-4 \frac{1}{n} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k=1}^n E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu) ] \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu)^2] \\
&- \frac{2}{n} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k=1}^n E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu) ] \\
& + \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)] \label{proof_variance_origin3}
\end{align}
Term 1
\begin{align*}
& \sum_{i = 1}^{n} \sum_{j = 1}^{n} E[(x_i - \mu)^2 (x_j - \mu)^2] \\
&=\sum_{i = 1}^{n} E[(x_i - \mu)^4] + \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2 (x_j - \mu)^2]
\end{align*}
\begin{align*}
& n + n (n-1) \\
&= n^2
\end{align*}
Term 2
\begin{align*}
& \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k=1}^n E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu) ] \\
&=\sum_{i=1}^n E[(x_i - \mu)^4] \\
&+\sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2(x_j - \mu)^2]
+2\sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)(x_j - \mu)^3] \\
&+\sum_{\substack{\text{distinct} \\ i,j,k}} E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu)]
\end{align*}
\begin{align*}
& n + n (n-1) + 2 n (n-1) + n (n-1)(n-2) \\
&= n + 3 n (n-1) + n (n-1)(n-2) \\
&= n + n(n-1)(n+1) \\
&= n + n(n^2-1) \\
&= n + n^3 - n \\
&= n^3
\end{align*}
Term 3
\begin{align*}
& \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)] \\
&=\sum_{i=1}^n E[(x_i - \mu)^4] \\
&+ 3 \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2 (x_j - \mu)^2]
 + 4 \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu) (x_j - \mu)^3] \\
&+ 6 \sum_{\substack{\text{distinct} \\ i,j,k}} E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu)] \\
&+ \sum_{\substack{\text{distinct} \\ i,j,k,l}} E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)]
\end{align*}
\begin{align*}
& n + 3 n (n-1) + 4 n (n-1) + 6 n (n-1)(n-2) +  n (n-1)(n-2)(n-3) \\
&= n + 7 n (n-1) + 6 n (n-1)(n-2) +  n (n-1)(n-2)(n-3) \\
&= n + 7 n (n-1) + n (n-1)(n-2)(n+3) \\
&= n + 7 n (n-1) + n (n-1)(n^2 +n -6) \\
&= n + n (n-1)(n^2 +n +1) \\
&= n + n (n^3 -1) \\
&= n + n^4 -n \\
&= n^4
\end{align*}
\begin{align*}
\sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu) (x_j - \mu)^3]
&=\sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)] E[(x_j - \mu)^3] \\
&=\sum_{\substack{\text{distinct} \\ i,j}} 0 \cdot E[(x_j - \mu)^3] \\
&= 0
\end{align*}
\begin{align*}
\sum_{\substack{\text{distinct} \\ i,j,k}} E[(x_i - \mu)^2 (x_j - \mu) (x_k - \mu)]
&= \sum_{\substack{\text{distinct} \\ i,j,k}} E[(x_i - \mu)^2] E[(x_j - \mu)] E[(x_k - \mu)] \\
&= \sum_{\substack{\text{distinct} \\ i,j,k}} E[(x_i - \mu)^2] \cdot 0 \cdot 0 \\
&= 0
\end{align*}
\begin{align*}
\sum_{\substack{\text{distinct} \\ i,j,k,l}} E[(x_i - \mu)(x_j - \mu)(x_k - \mu)(x_l - \mu)]
&= \sum_{\substack{\text{distinct} \\ i,j,k,l}} E[(x_i - \mu)] E[(x_j - \mu)] E[(x_k - \mu)] E[(x_l - \mu)] \\
&= \sum_{\substack{\text{distinct} \\ i,j,k,l}} 0 \cdot 0 \cdot 0 \cdot 0 \\
&= 0
\end{align*}
Equation \ref{proof_variance_origin3} becomes
\begin{align*}
E \Bigg[ \Bigg( \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg]
&= \sum_{i = 1}^{n} E[(x_i - \mu)^4] + \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2 (x_j - \mu)^2] \\
&- \frac{2}{n} \Bigg( \sum_{i=1}^n E[(x_i - \mu)^4] + \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2(x_j - \mu)^2] \Bigg) \\
&+ \frac{1}{n^2} \Bigg( \sum_{i=1}^n E[(x_i - \mu)^4] + 3 \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2 (x_j - \mu)^2] \Bigg) \\
&= \frac{n^2-2n+1}{n^2} \sum_{i = 1}^{n} E[(x_i - \mu)^4] + \frac{n^2-2n+3}{n^2} \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2 (x_j - \mu)^2]
\end{align*}
\begin{align*}
E[(x_i - \mu)^4]
&= \int_{-\infty}^{\infty} (x - \mu)^4 P(x) dx \\
&= \int_{-\infty}^{\infty} (x - \mu)^4 \frac{1}{\sigma \sqrt{2\pi}} \exp \Bigg[ - \frac{(x-\mu)^2}{2\sigma^2} \Bigg] dx \\
&= \int_{-\infty}^{\infty} (x - \mu)^4 \frac{1}{\sigma \sqrt{2\pi}} \exp \Bigg[ - \Bigg( \frac{x-\mu}{\sqrt{2}\sigma} \Bigg)^2 \Bigg] dx
\end{align*}
\begin{align*}
\text{Let} \qquad t &= \frac{x-\mu}{\sqrt{2}\sigma} \\
E[(x_i - \mu)^4] &= \int_{-\infty}^{\infty} (\sqrt{2}\sigma t)^4 \frac{1}{\sigma \sqrt{2\pi}} \exp (-t^2) (\sqrt{2}\sigma dt) \\
&= \frac{4\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} t^4  e^{-t^2} dt \\
&= -\frac{2\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} t^3 (-2t e^{-t^2}) dt \\
&= -\frac{2\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} t^3 d(e^{-t^2}) \\
&= -\frac{2\sigma^4}{\sqrt{\pi}} \Bigg( [t^3 e^{-t^2}]_{-\infty}^{\infty} -\int_{-\infty}^{\infty} 3t^2 e^{-t^2} dt \Bigg) \\
&= \frac{6\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} t^2 e^{-t^2} dt \\
&= -\frac{3\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} t (-2t e^{-t^2}) dt \\
&= -\frac{3\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} t d(e^{-t^2}) \\
&= -\frac{3\sigma^4}{\sqrt{\pi}} \Bigg( [t e^{-t^2}]_{-\infty}^{\infty} -\int_{-\infty}^{\infty} e^{-t^2} dt \Bigg) \\
&= \frac{3\sigma^4}{\sqrt{\pi}} \int_{-\infty}^{\infty} e^{-t^2} dt \\
&= \frac{3\sigma^4}{\sqrt{\pi}} \sqrt{\pi} \\
&= 3\sigma^4
\end{align*}
\begin{align*}
\sum_{i = 1}^{n} E[(x_i - \mu)^4]
&=\sum_{i = 1}^{n} 3\sigma^4 \\
&= 3n \sigma^4
\end{align*}
\begin{align*}
\sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2 (x_j - \mu)^2]
&= \sum_{\substack{\text{distinct} \\ i,j}} E[(x_i - \mu)^2] E[(x_j - \mu)^2] \\
&= \sum_{\substack{\text{distinct} \\ i,j}} \sigma^2 \sigma^2 \\
&= \sum_{\substack{\text{distinct} \\ i,j}} \sigma^4 \\
&= n(n-1) \sigma^4
\end{align*}
\begin{align*}
E \Bigg[ \Bigg( \sum_{i = 1}^{n} (x_i - \bar{x})^2 \Bigg)^2 \Bigg]
&= \frac{n^2-2n+1}{n^2} (3n \sigma^4) + \frac{n^2-2n+3}{n^2} [n(n-1) \sigma^4] \\
&= \frac{3(n-1)^2}{n} \sigma^4 + \frac{(n^2-2n+3)(n-1)}{n} \sigma^4 \\
&= \frac{(n-1)[(3n-3)+(n^2-2n+3)]}{n} \sigma^4 \\
&= \frac{(n-1)(n^2+n)}{n} \sigma^4 \\
&= (n-1)(n+1) \sigma^4 \\
\end{align*}
Equation \ref{proof_variance_origin1} becomes
\begin{align*}
\sigma_{\hat{\sigma^2}}^2
&= \frac{1}{(n-1)^2} [ (n-1)(n+1) \sigma^4 ] - \sigma^4 \\
&= \frac{n+1}{n-1} \sigma^4 - \sigma^4 \\
&= \frac{[(n+1)-(n-1)]}{n-1} \sigma^4 \\
&= \frac{2\sigma^4}{n-1}
\end{align*}

\medskip

\begin{thebibliography}{99}

\bibitem{stat_1}
James F. (2008). Statistical Methods in Experimental Physics.

\bibitem{stat_2}
Leo W. R. (1994). Techniques for Nuclear and Particle Physics Experiments. Pages 81 - 113

\bibitem{stat_3}
Taylor J. R. (1997). An Introduction to Error Analysis. 

\bibitem{BAKER1984437}
Steve Baker and Robert D. Cousins (1984). Clarification of the use of CHI-square and likelihood functions in fits to histograms. Nuclear Instruments and Methods in Physics Research, Volume 221, Issue 2, Pages 437 - 442 

\bibitem{reduced_chi_squared}
R. Andrae, T. Schulze-Hartung, P. Melchior (2010). Dos and don’ts of reduced chi-squared. arXiv:1012.3754v1. 

\end{thebibliography}



\end{document}
