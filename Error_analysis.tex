\documentclass[12pt,a4paper]{report}
%\documentstyle[epsfig]{article}
\textheight=700pt
\textwidth=510pt
\oddsidemargin=-30pt
\topmargin=-35pt
\usepackage{graphicx,wrapfig,epstopdf}
\usepackage[nottoc]{tocbibind}
\usepackage[]{amsmath}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textgreek}
\usepackage{csquotes}
\usepackage{titlesec}
\usepackage{breqn}
\usepackage{hyperref}
\setcounter{chapter}{1}

\begin{document}
\begin{center}
{\bf Advanced Lab's course} \\ %\vspace{2mm}
Introduction to Statistical Analysis \\
and Error Analysis \\
\end{center}


\section{Aim of the lab}
This lab allows students to meet several goals:
\begin{enumerate}
\item Introduction to error analysis and statistical analysis of the data
\item Introduction to basics of analysis and simulations with \textbf{Matlab} (or any other software used to do data analysis)
\item Performance of basic measurements of data with low are large statistics
\end{enumerate}


\section{Introduction}
Data analysis and statistical methods play an essential role in science as it is a tool which gives possibility to treat uncertainties to data and draw conclusions. For experimental physics, it is also a tool to design and plan an experiment. Statistical methods of data analysis allow to identify independent and depended variables in the system, decide which model (theory) should be chosen for data description and etc. This laboratory manual is aimed to give students a brief introduction to data analysis techniques, however it is highly recommended to read further literature \cite{stat_1, stat_2, stat_3}.


\section{Elements of probability theory: distribution, moments of distribution, types of distribution}
During the experiment one deals with stochastic processes, during which each next trial is independent on previous result. Random processes are described by the \textit{probability density} function $P(x)$ (term \textit{distribution} function is also used). Depending on the nature of variable \enquote{x}, distribution $P(x)$ can be continuous or discrete. If variable \enquote{x} is discrete, then the probability to obtain \enquote{$x_i$} equals to $P(x_i)$. If variable \enquote{x} is continuous, then the probability to obtain \enquote{x} in the region from $x$ to $x+dx$ equals to $P(x)dx$. \\
Usually, distribution probability is normalised to 1. In case of continuous distribution:
\begin{equation} \label{norm_P_1}
\int_{-\infty}^{+\infty} P(x)dx = 1
\end{equation}
In case of discrete distribution:
\begin{equation} \label{norm_P_2}
\sum_{i} P(x_i) = 1
\end{equation}
The probability $p(x)$ to find $x$ between certain values in case of continuous distribution is defined in the following way:
\begin{equation} \label{prob_find_1}
p(x_1 \leq x \leq x_2) = \int_{x_1}^{x_2} P(x)dx
\end{equation}
In case of discrete distribution, integration should be replaced with summation:
\begin{equation} \label{prob_find_2}
p(x_1 \leq x \leq x_2) = \sum_{x = x_1}^{x_2} P(x)
\end{equation}

\subsection{Moments of distribution}
Theoretical mean value $\mu$ of variable $x$ is defined as the first moment about zero of the distribution $P(x)$. Mathematically it can be expressed in the following way:
\begin{equation} \label{first_moment}
\mu = \int x P(x) dx
\end{equation}
The variance of the distribution $\sigma^2$ is defined as a second moment about the mean value:
\begin{equation} \label{second_moment}
\sigma^2 = \int (x-\mu)^2P(x)dx
\end{equation}
It has a meaning of average squared deviation of variable \enquote{x} from its mean value $\mu$. Square root of variance, $\sigma$, is called standard deviation, and used to describe the width of the distribution. Value of $\sigma$ allows to judge about the magnitude of fluctuation of random variable \enquote{x} around its mean value $\sigma$. Also one can introduce higher moments of distribution, but they are rarely used in experimental physics. In case of discrete distributions, integration in equations (\ref{first_moment}, \ref{second_moment}) should be changed to summation.
\begin{equation}
\mu = \sum_i x_i P(x_i)
\end{equation}
\begin{equation}
\sigma^2 = \sum_i (x_i-\mu)^2P(x_i)
\end{equation}
Students are welcome to study literature for more information about it \cite{stat_1, stat_2, stat_3}. \\
The correlation (dependency) between parameters $x$ and $y$ is described by covariance:
\begin{equation} \label{cov}
cov(x,y) = \int (x-\mu_x)(y-\mu_y)P(x,y)dxdy
\end{equation}
where $P(x,y)$ is a 2-dimensional distribution of variables \enquote{x} and \enquote{y}. It is also convenient to introduce correlation coefficient $\rho$ ($\rho \in [-1; 1]$):
\begin{equation} \label{rho}
\rho = \frac{cov(x,y)}{\sigma_x \sigma_y}
\end{equation}
The value of $\rho$ gives information about the dependency between parameters. If $|\rho| = 1$, then it is a case of perfect linear correlation. If $\rho = 0$, variables are independent.

\subsection{Types of distributions: Binomial, Poisson, Gaussian} 
\textbf{The Binomial distribution} describes processes in which outcome of a trial is dichotomous (\textit{yes} or \textit{no}, \textit{head} or \textit{tail} and etc.). The probability of $r$ positive (or negative) outcomes in $N$ trials is calculated in the following way:
\begin{equation} \label{binom_dist}
P(r) = \frac{N!}{r! (N-r)!}p^r (1-p)^{N-r}
\end{equation}
where p is the probability of success (or failure) of one single trial. Examples of binomial distribution can be found in Fig.~\ref{fig:Binom}. It is easy to see that binomial distribution is discrete. Theoretical mean value $\mu$ equals to:
\begin{equation} \label{binom_mean_val}
\mu = \sum_r r P(r) = Np
\end{equation}
The variance of the distribution equals to:
\begin{equation} \label{binom_var}
\sigma^2 = \sum_r (r-\mu)^2P(r) = Np(1-p)
\end{equation}
Binomial distribution can be approximated with Poisson or Gaussian distributions, depending on the parameters. 
\begin{figure}[!h]
\begin{center}
\includegraphics*[scale = 0.5]{Figures/Binom}
\caption{\label{fig:Binom} Examples of binomial distribution with different parameter \enquote{$p$}. Details can be found in the legend.}
\end{center}
\end{figure}
\newpage
\textbf{The Poisson distribution} rises as a limit case of Binomial distribution, when the following conditions are satisfied:
\begin{enumerate}
\item Events are independent within any time interval.
\item Probability of one event $p \rightarrow 0$, while the amount of trials $N \rightarrow \infty$. 
\item Mean value $\mu = Np$ stays finite.
\end{enumerate}
Probability to observe $r$ events under the conditions listed above is calculated in the following way:
\begin{equation} \label{poisson_dist}
P(r) = \frac{\mu^r e^{-\mu}}{r!}
\end{equation}
Poisson distribution has many examples in every day life, for example, the radioactive decay of $^{238}$U isotope. This nucleus is an alpha-decayer with half-life $T_{1/2} \approx 4.5 \times 10^9$ years. From laws of nuclear physics, it is known that nuclei act independently from each other during radioactive decay. The probability of one uranium nucleus to decay in one second is $\lambda = \ln2 /T_{1/2} \approx 5 \times 10^{-18}$ s$^{-1}$. As one can see, probability of one single event is extremely small. But, suppose that we have 1 g of $^{238}$U, which contains $N \approx 2.5 \times 10^{21}$ isotopes, the total number of decay events in one second can be large and can be approximately described by a binomial distribution with $p = 5 \times 10^{-18}$ s$^{-1}$ and $N = 2.5 \times 10^{21}$. By equation \ref{binom_mean_val}, the mean value of the total number of decay events in one second is $\mu = Np \approx 12.5 \times 10^3$ decays/s $ = 12.5 $ decays/ms. If we want a distribution function of the total number of decay events in one millisecond, the Poisson distribution perfectly describes this situation. \par
\begin{figure}[!h]
\begin{center}
\includegraphics*[scale = 0.4]{Figures/Poisson}
\caption{\label{fig:Poisson} Examples of Poisson distribution with different parameter $\mu$. Details can be found in the legend.}
\end{center}
\end{figure}
Poisson distribution is discrete and has only one parameter $\mu$, which stands for the mean value. It can be shown, that variance of the distributions equals to:
\begin{equation}
\sigma^2 = \mu 
\end{equation}
On Fig.~\ref{fig:Poisson} one can find several examples of Poisson distribution with different values of $\mu$. As it is seen, with small values of $\mu$, Poisson distribution is asymmetric. But with relatively large values of $\mu$ distributions becomes symmetric and can be approximated by the Gaussian distribution. During such approximation, the discrete nature of Poisson distribution can be neglected.  \par
\textbf{The Gaussian or normal distribution} is one of the most important distributions in statistics and for data analysis in particular. As it was mentioned above, binomial and Poisson distributions can be approximated by Gaussian distribution. Also, it is known from central limit theorem (CLT), that if a set of independent random variables (even with distributions different from normal) are added, their sum will tend to normal distribution. On practice, it means that many processes can be approximated with Gaussian distribution. For example, instrumental errors are generally distributed normally.
The Gaussian distribution is a symmetric, continuous distribution, with density given by:
\begin{equation} \label{gauss_dist}
P(x) = \frac{1}{\sigma \sqrt{2\pi}}exp\Big ( - \frac{(x-\mu)^2}{2\sigma^2} \Big )
\end{equation}
It is dependent of two parameters: mean value $\mu$ and standard deviation $\sigma$. Example of Gaussian distribution can be found in Fig.~\ref{fig:Gaussian}. Parameter $\sigma$ characterises the width of Gaussian distribution. It is also common to use another parameter to describe the width, the full width of half maximum (FWHM). It can be shown, that:
\begin{equation} \label{FWHM}
\text{FWHM} = 2 \sqrt{2\ln 2}\sigma \approx 2.35 \sigma
\end{equation}
\begin{figure}[!t]
\begin{center}
\includegraphics*[scale = 0.4]{Figures/Gauss}
\caption{\label{fig:Gaussian} Examples of Gaussian distribution with different parameters $\sigma$. Details can be found in the legend.}
\end{center}
\end{figure}
\begin{figure}[!t]
\begin{center}
\includegraphics*[scale = 0.3]{Figures/Gauss_intervals}
\caption{\label{fig:Gauss_intervals} Area under Gaussian distribution, contained between different intervals. Figure is taken from \cite{stat_2}.}
\end{center}
\end{figure}
Another important feature of normal distribution is the area under it, depending on the intervals. Example can be found in Fig.~\ref{fig:Gauss_intervals}. The area in the interval $[\mu-\sigma;\mu+\sigma]$ equals to $\approx 68\%$. For the interval within $\pm 2\sigma$ and $\pm 3\sigma$, the areas are $95.5\%$ and $99.7\%$ respectively. This result is very important for data representation and means that if one will present experimental results within only 1$\sigma$, then there is approximately a probability of 1/3 that the true value will be outside the region. If results are presented with 2$\sigma$ interval, this probability is less than 5\%.



\section{Statistical methods in experimental physics} \label{methods}

In general case, he estimation of the population mean of a sample $x \in \{ x_1, x_2, x_3, .... , x_n \}$ is calculated in the following way:
\begin{equation} \label{av_value}
\bar{x} = \frac{1}{n} \sum_ i {x_i}
\end{equation}
The estimation of the population standard deviation $s$ is given by:
\begin{equation}
s = \sqrt{\frac{1}{(n-1)} \sum_i {(x_i - \bar{x})^2}}
\end{equation}

The estimated standard deviation of $\bar{x}$ is given by:
\begin{equation} \label{standart_dev}
\sigma_{\bar{x}} = \frac{s}{ \sqrt{n} }
\end{equation}

The estimated mean $\bar{x}$ can be described by the normal distribution with the mean $\bar{x}$ and standard deviation $s / \sqrt{n}$, if the sample size is large, typically $n>30$.
Standard deviation $\sigma_{\bar{x}}$ has a meaning of a statistical error and gives a probability of 66$\%$ that the population mean $\mu$ will be found in the region $\bar{x} - \sigma_{\bar{x}} \le x \le \bar{x} + \sigma_{\bar{x}}$. In physics, confidence interval is usually within $2 \sigma$ with the confidence level $\alpha = 95 \%$
\begin{equation} \label{confidence_interval_large}
\bar{x} - 2 \sigma_{\bar{x}} \le x \le \bar{x} + 2 \sigma_{\bar{x}}
\end{equation}

If the sample size is small, $n<30$, we need to use the student's $t$ coefficient $t_{\alpha,n}$.
Its value depends on the sample size n and the confidence level $\alpha$, and their values can be found from the table \ref{table:student_t_value} with the confidence level $\alpha = 95 \%$. The confidence interval with confidence level $\alpha = 95 \%$ is given by
\begin{equation}
\bar{x} - t_{0.95, n} \sigma_{\bar{x}} \le x \le \bar{x} + t_{0.95, n} \sigma_{\bar{x}}
\end{equation}
When $n>30$, $t_{0.95, n}$ is reduced to 2 and the confidence interval is reduced to \ref{confidence_interval_large}.


\begin{table}[!h]
\begin{center}
\caption{\label{table:student_t_value} Values of student's $t$ coefficient for confidence level $\alpha$ = 95$\%$.}
\begin{tabular}{l l l l l l l l l}
\hline
n & 2 & 3 & 4 & 5 & 6 & 7-8 & 9-10 & 30-$\infty$  \\
$t_{0.95, n}$ &12.7 & 4.3 & 3.2 & 2.8 & 2.6 & 2.4 & 2.3 & 2 \\
\hline
\end{tabular}
\end{center}
\end{table}

During the experiment, every quantity contains two types of error: statistical and systematical. Final result for value $x$ is written in the following way:
\begin{equation} \label{final_value}
x = \bar{x} \pm \sqrt{\sigma_{stat}^2+\sigma^2_{syst}}
\end{equation}
 
\subsection{Propagation of Errors} \label{error_analysis}
This section covers a method to estimate the total error $\sigma_{f}$ of a function $f=f(x,y,z)$, when individual error of variables x, y and z are known, namely $\sigma_x$, $\sigma_y$ and $\sigma_z$. It is considered that variables x, y and z are independent. It can be shown that $\sigma_{f}$ can be expressed in the following way:
\begin{equation} \label{error_propagation}
\sigma_{f} = \sqrt{\Big ( \frac{\partial f}{\partial x} \sigma_x \Big )^2+\Big ( \frac{\partial f}{\partial y} \sigma_y \Big )^2+\Big ( \frac{\partial f}{\partial z} \sigma_z \Big )^2}
\end{equation} 
In more general case, if variables x, y and z are dependent, the covariances between variables must be taken into account. Additional information on this case can be found in \cite{stat_1, stat_2, stat_3}.

For example, if $f(x,y,z) = x+y+z$, their partial derivatives are 1. The total error is
\begin{equation}
\sigma_{f} = \sqrt{( \sigma_x )^2 + ( \sigma_y )^2+ ( \sigma_z )^2}
\end{equation}
 This means that if $f$ is the sum of x, y and z, the resulting error $\sigma_{f}$ is the square root of the sum of squares of individual errors.
 
Consider the case that $f(x,y,z) = xyz$. The total error is
\begin{align}
\sigma_{f} &= \sqrt{( yz \sigma_x )^2 + ( xz \sigma_y )^2+ ( yz \sigma_z )^2} \\
\frac{ \sigma_{f} }{xyz} &= \sqrt{ \Big( \frac{\sigma_x}{x} \Big)^2 + \Big( \frac{\sigma_y}{y} \Big)^2 + \Big( \frac{\sigma_z}{z} \Big)^2} \\
\frac{ \sigma_{f} }{f} &= \sqrt{ \Big( \frac{\sigma_x}{x} \Big)^2 + \Big( \frac{\sigma_y}{y} \Big)^2 + \Big( \frac{\sigma_z}{z} \Big)^2}
\end{align}
This means that if $f$ is the product of x, y and z, the resulting relative error $\sigma_{f}/f$ is the square root of the sum of squares of individual relative errors.

\section{Fitting methods}
\subsection{Likelihood function method}

\subsection{$\chi^2$ method}

$\chi^2$ analysis \cite{stat_2, BAKER1984437, reduced_chi_squared} is based on minimising the difference between experimental data and the fitting function:

\begin{equation} \label{chi_square_formula}
\chi^2 = \sum_{j = 1}^{n} \Big ( \frac{y_j - F(E_j,\vec{\theta})}{\sigma_j} \Big )^2 
\end{equation}

where summation is taken over all experimental points. In formula \ref{chi_square_formula} $y_j$ stands for experimental data points, $\sigma_j$ - standard deviation (error) of experimental data, $F(x_j,\vec{\theta})$ - fitting function with a set of parameters $\vec{\theta}$. Parameters $\theta$ are found from the minimisation condition:

\begin{equation} \label{chi_minimisation_condition}
\frac{\partial \chi^2}{\partial \theta_j} = 0
\end{equation}

\subsection{Analysis of data in case of large statics}
The technique, discussed at the beginning of section \ref{methods}, can be used always. But the case of large statistics of experimental data gives access to analyse the data via fitting. On Fig.~\ref{fig:radius_distribution} one can see the examples of the distribution of a radius for a particle and square displacement of a Brownian particle. Large statistics of data allows to build the distribution of a value $x$. Mean value $\bar{x}$ and standard deviation $\sigma_{\bar{x}}$ of mean value $\bar{x}$ are estimated from the fit of the distribution. According to Central limit theorem a set of independent random variables tends toward a normal (Gauss) distribution. On the Fig.~\ref{fig:radius_distribution} one can find distribution of the value of radius $r$ of Brownian particle and square displacement $\langle x^2 \rangle$.
\begin{figure}[!h]
\begin{center}
\includegraphics*[scale = 0.25]{Figures/two_plots}
\caption{\label{fig:radius_distribution} Example of the distribution of a radius for a particle and square displacement of a Brownian particle. Data is generated using Gauss distribution in both cases.}
\end{center}
\end{figure}
Final result with confidence interval of $\alpha = 66 \%$ is written in the following way:
\begin{equation}
x = \bar{x} \pm \sqrt{\sigma_{stat}+\sigma_{syst}}
\end{equation}
Final result with confidence interval of $\alpha = 95 \%$ is written in the following way:
\begin{equation} \label{error_offline}
x = \bar{x} \pm \sqrt{2\sigma_{stat}+\sigma_{syst}}
\end{equation}



\section{Experimental Setup and Procedure}
Experimental equipment consists from following parts:
\begin{enumerate}
\item Standard micrometer
\item Thin aluminium plate
\end{enumerate}
Below a student can find a list of \enquote{must-do} tasks, but students are highly motivated to expand to data analysis further:

\begin{enumerate}
\item Measure the thickness of a thin aluminium plate, using standard micrometer. Obtain data of large (200 points) and low (10 points) statistics. Estimate mean value (\ref{av_value}) and standard deviation (\ref{standart_dev}) for each case. Fit each data set with a distribution function, extract mean value and standard deviation from the fitting. Calculate $\chi^2$ (\ref{chi_square_formula}) for each case and define \enquote{goodness} of the fit. Present your results with 95$\%$ confidence level, taking into account statistical and systematical errors. 
\item Simulate Landau distribution sitting on a background curve (exponential decay). Add statistical fluctuations and error bars (take as $\sqrt{N}$) to data points. Fit the data, estimate \enquote{goodness} of the fit, extract parameters of the distribution and compare them with the simulated values. Present your results with 95$\%$ confidence level, taking into account statistical and systematical errors.
\item Simulate two Gaussian distributions party merging with each other, and sitting on the background curve (exponential decay). Simulate several cases with Gaussians of the same and different statistics, different widths of Gaussian distributions. Fit the data, estimate \enquote{goodness} of the fit, extract parameters of the distribution and compare them with the simulated values for each case. Present your results with 95$\%$ confidence level, taking into account statistical and systematical errors.
\end{enumerate}

\section{Discussion}
To successfully pass the lab, a student must complete the following tasks:
\begin{enumerate}
\item What is the distribution function of the thickness of aluminium plate? How many parameters does it have? 
\item In what cases does the $\chi^2$ analysis method can be used? Derive $\chi^2$ analysis method from maximum likelihood method.
\item Other questions
\end{enumerate}

\medskip

\begin{thebibliography}{99}

\bibitem{stat_1}
James F. (2008). Statistical Methods in Experimental Physics.

\bibitem{stat_2}
Leo W. R. (1994). Techniques for Nuclear and Particle Physics Experiments. p. 81

\bibitem{stat_3}
Taylor J. R. (1997). An Introduction to Error Analysis. 

\bibitem{BAKER1984437}
Steve Baker and Robert D. Cousins (1984). Clarification of the use of CHI-square and likelihood functions in fits to histograms. Nuclear Instruments and Methods in Physics Research, Volume 221, Issue 2, Pages 437 - 442 

\bibitem{reduced_chi_squared}
R. Andrae, T. Schulze-Hartung, P. Melchior (2010). Dos and donâ€™ts of reduced chi-squared. arXiv:1012.3754v1. 

\end{thebibliography}



\end{document}
